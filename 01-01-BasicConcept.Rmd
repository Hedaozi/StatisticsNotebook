# (PART) 初级统计 {-}

# 基本统计概念 {#BasicConcept}

## 大数定律与中心极限定律 {#CentralLimitTheorem}

用通俗的语言讲，大数定律描述这样一种现象：多次重复实验的算术平均数（可以理解为样本均值）随样本量增加收敛于总体的期望。

中心极限定律则描述：对独立重复试验，样本均值**依分布**收敛于一正态分布，该正态分布的期望即为总体期望，方差为总体方差除以样本量。由此，在样本量趋向无穷大的过程中，**均值的标准差**（即**抽样误差**）趋向于0。进而，有如下直观描述：当样本量很大的时候，样本均值十分接近总体均值，因为偏差的程度非常小。

以下是一个关于中心极限定律的模拟。若要获得更好的交互体验，请点击[此处](http://shiny.hedaozi.com/clt/)。

```{r echo=FALSE}
knitr::include_app("http://shiny.hedaozi.com/clt-mini/", height = 900)
```

## 总体分布，样本分布，抽样分布 {#DistributionOfPopulationSampleSampling}

总体分布（population distribution），指涉总体中个体的分布函数。当总体不变时，总体分布不变。

样本分布（sample distribution），指涉样本中个体的分布函数，与总体分布相近。样本分布随实际抽出的样本变化而变化。

抽样分布（sampling distribution），指涉样本统计量的分布函数。所谓抽样误差，指涉某一样本统计量的标准差。例如对同一总体，以同样的方式进行抽样，每次抽样的样本均值本身亦是随机变量，该随机变量的分布函数即是均值的抽样分布；又如样本方差。

## 常用抽样分布：$\chi^2$分布，$F$分布与$t$分布 {#CommonSamplingDistribution}

先介绍一下自由度的概念。直观理解，自由度就是已知样本统计量的观测值时，可以自由取值的个体数量。例如，已知样本均值，则样本中$n-1$个个体可以自由取值，而最后一个个体的观测值可以经由前$n-1$个个体的观测值与样本均值计算而出，因此此时的自由度为$n-1$。

### $\chi^2$分布的直观含义及性质 {#ChiSquareDistribution}

直观地理解，$\chi^2$是标准正态分布的平方和。因此，$\chi^2$一般用以做方差的假设检验。

其数学表述为：在标准正态总体$X\sim N(0,1)$中，取样本$(X_1,X_2,…,X_n)$，则

$$Y=∑_{i=1}^nX_i^2\sim χ^2(n)$$

$\chi^2$分布具有可加性，即若$X\sim \chi^2(n)$，$Y\sim \chi^2 (m)$，且$X$、$Y$相互独立，那么有

$$Z=X+Y\sim \chi^2(n+m)$$

### $F$分布的直观含义及性质 {#FDistribution}

直观地理解，$F$分布是$\chi^2$的均值的比。因此，$F$分布一般用以作方差比的假设检验。

其数学表述为：若$X\sim \chi^2(n)$，$Y\sim \chi^2 (m)$，且$X$、$Y$相互独立，则：

$$Z=\frac{X/n}{Y/m}\sim F(n,m)$$

即$F$分布可以看作是两个服从$\chi^2$分布且相互独立的随机变量除以各自的自由度后再相除，被除者的自由度即为第一自由度，除者的自由度即为第二自由度，由此可知：

$$Z\sim F(n,m) \Leftrightarrow \frac{1}{Z}\sim F(m,n)$$

###  $t$分布的直观含义及性质 {#TDistribution}

直观地理解，$t$分布是标准正态分布除以$\chi^2$的均值的算术平方根。因此，$t$分布一般用于总体方差未知的均值检验。

若$X\sim N(0,1)$，$Y\sim \chi^2(n)$，且$X$、$Y$相互独立，则

$$Z=\frac{X}{\sqrt{Y/n}}\sim t(n)$$

当$n$足够大时，可用标准正态分布作近似估计，因为

$$t(n) \xrightarrow {n\rightarrow\infty} N(0,1)$$

## 点估计与区间估计 {#PointAndIntervalEstimation}

点估计与置信区间皆为通过样本估计总体参数的方法。由于经过构造的随机样本的统计量的期望等于总体参数，因此可以通过样本统计量估计总体参数。

$$样本统计量 \xrightarrow {大数定律\rightarrow 收敛于期望} 点估计\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $$

$$样本统计量 \xrightarrow {概\ \ \ \ \ \ \ \ 率\ \ \ \ \ \ \ \ 模\ \ \ \ \ \ \ \ 型} 区间估计与假设检验$$

点估计即通过样本统计量估计总体参数，由于对任一给定样本，统计量的观测值唯一，因而叫做点估计。

区间估计，则构造一个区间，使得总体参数落在该区间的概率为一给定的概率。区间估计需要知道样本统计量的分布函数。一般而言，点估计值即为区间的中点。估计区间即为**置信区间**。给定的概率水平即为**置信水平**。总体参数落入置信区间为一随机01事件，该事件发生的概率即为置信水平。

## 假设检验思想与$p$值 {#HypothesisTestingAndPValue}

假设检验的基本思想类同反证法：认为小概率事件不可能发生。假设检验需要给出一组假设，包含原假设与备择假设。如果原假设下的小概率事件发生了，那么就拒绝原假设。

假设检验需要有一给定的小概率事件的界定（即需给定显著性水平$\alpha$，发生概率低于$\alpha$的事件即为小概率事件）。并且，需要比较样本统计量及其更极端取值情况的概率与显著性水平。只有当样本的检验统计量的分布已知时，才能计算出样本统计量取到某值及更极端值的概率。所谓更极端值，也就是更偏离原假设的值。检验统计量必须是若干样本统计量与一个未知总体参数构成的，并且其分布已知。

由于中心极限定律保证了大样本下，样本均值依分布收敛于正态分布。因此，对任意随机大样本，其均值都可以做单样本均值假设检验。

所谓$p$值，直观地理解，即为在原假设下，统计量取到该值或更极端情况的值的概率。$p$值根据备择假设的方向改变“更极端情况”的定义，因此在不同备择假设下，$p$值会有所不同。
