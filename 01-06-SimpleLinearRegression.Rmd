# 一元线性回归 {#SimpleLinearRegression}

线性回归的各自变量系数，是用最小二乘法计算得到的。所谓最小二乘法，直观理解，就是寻找到一列的系数$β_0,β_1,β_2,…,β_m$，使得由这一系列系数确定的回归直线

$$y=β_0+\sum_{i=1}^mβ_ix_i+\epsilon$$

能解释最多的差异。

任何一组数据，都能够求出一条对应的回归直线。但是回归直线是否有用，需要经过检验。首先是回归直线本身的检验。这一检验类似于方差分析，利用$F$统计量。回归直线本身的检验的思想在于，因变量$Y$的方差可以分解为两部分，一部分为回归直线解释，另一部分为剩余的未被解释的。如果回归直线解释的部分较多，那么可以认为回归直线有统计显著性，反之则认为无。这一检验，要求因变量$Y$服从正态分布。

其次是因变量与各因变量的线性相关性的检验。如果自变量与因变量无线性相关性，则需要从模型中剔除之。在多元线性回归中，因变量与各自变量的相关性会用偏相关分析（目的在于消除共线性的影响），这也是为什么多元回归中，报告的线性相关系数可能会与两个变量直接用相关命令求得的线性相关系数不等的原因。
